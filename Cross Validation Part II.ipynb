{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation (Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "What is a classification problem?\n",
    "    Use features (X) to be models to predict an outcome/target (y).\n",
    "    \n",
    "What is a binary classification problem?  Classification, where y is binary.\n",
    "    Example: predicting acceptance to UCLA.\n",
    "    \n",
    "Binary classification is related to information retrieval.\n",
    "\n",
    "To validate the performance of a classifier, we use precision and recall.\n",
    "\n",
    "What is precision? and recall?\n",
    "\n",
    "Precision = TP / P.   This is the probabilty that a positive prediction is correct.\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Recall = TP / (TP + FN).  This is the probability that a correct data point is predicted.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "400 people are suspected to be infected with a virus. Through bloodwork, a new medical procedure predicted that 320 of these people were infected.\n",
    "\n",
    "In reality, only 280 of these 320 were actually infected.  Further, out of the 80 people who were thought not to be infected, 30 of them were infected.  The procedure failed to detect them.\n",
    "\n",
    "P = 320\n",
    "TP = 280\n",
    "FP = 40\n",
    "FN = 30\n",
    "\n",
    "Precision = 280 / 320\n",
    "\n",
    "Recall = 280 / (280 + 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification\n",
    "\n",
    "y is not binary.  y has more than 2 values.  Example: iris dataset.\n",
    "\n",
    "To validate, we average precision/recall across all classes.  For example, with respect to setosa, all setosa data points are considered \"positives\"; the others are \"negatives\".\n",
    "\n",
    "Per-class precision/recall is also valuable for binary classification.\n",
    "\n",
    "Example: admission.csv dataset.  We have to classes. Overall precison/recall, class 1 is positive. (admitted).  Class 0 is negative.\n",
    "\n",
    "If we consider per-class precision/recall, then for class 1, 1 is positive and 0 is negative; and for class 0, 0 is positive and 1 is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "df = pandas.read_csv('~/Dropbox/datasets/iris.csv')\n",
    "X = df.drop('Species', axis=1)\n",
    "y = df.Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SepalLength  SepalWidth  PetalLength  PetalWidth Species\n",
       "17          5.1         3.5          1.4         0.3  setosa"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([ [5.0, 3.4, 1.6, 0.5]  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate a decision tree model (on iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     setosa       1.00      1.00      1.00        50\n",
      " versicolor       1.00      1.00      1.00        50\n",
      "  virginica       1.00      1.00      1.00        50\n",
      "\n",
      "avg / total       1.00      1.00      1.00       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "model.predict(X)\n",
    "print(classification_report(y, model.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This validation is not good because it's done on training data (i.e the data we used to build the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proper way is to use one set for training (i.e. model building/fitting) and another set for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     setosa       1.00      1.00      1.00         4\n",
      " versicolor       0.86      1.00      0.92         6\n",
      "  virginica       1.00      0.80      0.89         5\n",
      "\n",
      "avg / total       0.94      0.93      0.93        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Validate the performance of decision tree on predicting acceptance to UCLA.  Use precision_score, recall_score as performance measures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.3157894736842105\n"
     ]
    }
   ],
   "source": [
    "# 1. get the data\n",
    "df = pandas.read_csv('~/Dropbox/datasets/admission.csv')\n",
    "\n",
    "# 2. get X and y\n",
    "y = df.admit\n",
    "X = df.drop('admit', axis=1)\n",
    "\n",
    "# 3. create the model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# 4. create training/testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# 5. build model.\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 6. validate model.\n",
    "y_pred = model.predict(X_test)\n",
    "print( precision_score(y_test, y_pred), recall_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: performance varies across different splits.\n",
    "\n",
    "Solution: cross validate.\n",
    "\n",
    "Challenge: write a Python function to cross validate a model across multiple random splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_validate(model, X, y, n, test_size):\n",
    "    p, r = [], []\n",
    "    for i in range(n):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        p.append( precision_score(y_test, y_pred) )\n",
    "        r.append( recall_score(y_test, y_pred) )\n",
    "    return round(sum(p)/n, 2), round(sum(r)/n, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4, 0.4)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(DecisionTreeClassifier(), X, y, 100, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.41318903318903316, 0.24706709956709955)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(DecisionTreeClassifier(max_depth=7), X, y, 10, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: which one is better?  Decision Tree or Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49, 0.35)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(RandomForestClassifier(n_estimators=50), X, y, 100, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different types of cross validation\n",
    "\n",
    "1. Shuffle and Split.  This is what we just did by going through multiple train_test_split.\n",
    "\n",
    "2. KFold.\n",
    "\n",
    "3. Repeated KFold.  (repeat KFold n times, each with a random initial ordering of data)\n",
    "\n",
    "4. Stratified KFold.  (similar to KFold but attempts to appropriate the distribution of the data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other measures of classification performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from precision and recall, people also look at sensitivity and specificity.\n",
    "\n",
    "Sensitivity is another name for recall.  TP / (TP + FN)\n",
    "\n",
    "Specificity is the true negative rate.  TN / (TN + FP)\n",
    "\n",
    "Just like Precision + Recall go \"together\",  sensitivity and specificity go together.  These measures are prefered in applications where true negative rates are important to know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
